# ITCS-6190 Assignment 3: AWS Data Processing Pipeline

This project demonstrates an end-to-end serverless data processing pipeline on AWS. The process involves ingesting raw data into S3, using a Lambda function to process it, cataloging the data with AWS Glue, and finally, querying and visualizing the results on a dynamic webpage hosted on an EC2 instance.

## 1. Amazon S3 Bucket Structure ü™£

First, set up an S3 bucket with the following folder structure to manage the data workflow:

* **`bucket-name/`**
    * **`raw/`**: For incoming raw data files.
    * **`processed/`**: For cleaned and filtered data output by the Lambda function.
    * **`enriched/`**: For storing athena query results.
      
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 3 35 19‚ÄØPM" src="https://github.com/user-attachments/assets/c79d125d-138b-4f1e-928e-d128f98c1bb6" />

---

## 2. IAM Roles and Permissions üîê

Create the following IAM roles to grant AWS services the necessary permissions to interact with each other securely.

### Lambda Execution Role

1.  Navigate to **IAM** -> **Roles** and click **Create role**.
2.  **Trusted entity type**: Select **AWS service**.
3.  **Use case**: Select **Lambda**.
4.  **Add Permissions**: Attach the following managed policies:
    * `AWSLambdaBasicExecutionRole`
    * `AmazonS3FullAccess`
5.  Give the role a descriptive name (e.g., `Lambda-S3-Processing-Role`) and create it.

### Glue Service Role

1.  Create another IAM role for **AWS service** with the use case **Glue**.
2.  **Add Permissions**: Attach the following policies:
    * `AmazonS3FullAccess`
    * `AWSGlueConsoleFullAccess`
    * `AWSGlueServiceRole`
3.  Name the role (e.g., `Glue-S3-Crawler-Role`) and create it.

### EC2 Instance Profile

1.  Create a final IAM role for **AWS service** with the use case **EC2**.
2.  **Add Permissions**: Attach the following policies:
    * `AmazonS3FullAccess`
    * `AmazonAthenaFullAccess`
3.  Name the role (e.g., `EC2-Athena-Dashboard-Role`) and create it.
   
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 3 35 42‚ÄØPM" src="https://github.com/user-attachments/assets/89bac615-2ef9-4c63-a83b-37121f016407" />

---

## 3. Create the Lambda Function ‚öôÔ∏è

This function will automatically process files uploaded to the `raw/` S3 folder.

1.  Navigate to the **Lambda** service in the AWS Console.
2.  Click **Create function**.
3.  Select **Author from scratch**.
4.  **Function name**: `FilterAndProcessOrders`
5.  **Runtime**: Select **Python 3.9** (or a newer version).
6.  **Permissions**: Expand *Change default execution role*, select **Use an existing role**, and choose the **Lambda Execution Role** you created.
7.  Click **Create function**.
8.  In the **Code source** editor, replace the default code with LambdaFunction.py code for processing the raw data.

<img width="1440" height="900" alt="Screenshot 2025-11-12 at 3 36 23‚ÄØPM" src="https://github.com/user-attachments/assets/029fe1a2-79ac-4f32-9d40-f0330019e729" />

---

## 4. Configure the S3 Trigger ‚ö°

Set up the S3 trigger to invoke your Lambda function automatically.

1.  In the Lambda function overview, click **+ Add trigger**.
2.  **Source**: Choose **S3**.
3.  **Bucket**: Select your S3 bucket.
4.  **Event types**: Choose **All object create events**.
5.  **Prefix (Required)**: Enter `raw/`. This ensures the function only triggers for files in this folder.
6.  **Suffix (Recommended)**: Enter `.csv`.
7.  Check the acknowledgment box and click **Add**.
   
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 3 36 01‚ÄØPM" src="https://github.com/user-attachments/assets/b0d2d8fe-e6e7-44d7-a0d5-d09bb8c94cf6" />

--- 
**Start Processing of Raw Data**: Now upload the Orders.csv file into the `raw/` folder of the S3 Bucket. This will automatically trigger the Lambda function.

<img width="1440" height="900" alt="Screenshot 2025-11-12 at 6 26 43‚ÄØPM" src="https://github.com/user-attachments/assets/b01d00ab-1514-451d-93c6-7b318ba91767" />

---

## 5. Create a Glue Crawler üï∏Ô∏è

The crawler will scan your processed data and create a data catalog, making it queryable by Athena.

1.  Navigate to the **AWS Glue** service.
2.  In the left pane, select **Crawlers** and click **Create crawler**.
3.  **Name**: `orders_processed_crawler`.
4.  **Data source**: Point the crawler to the `processed/` folder in your S3 bucket.
5.  **IAM Role**: Select the **Glue Service Role** you created earlier.
6.  **Output**: Click **Add database** and create a new database named `orders_db`.
7.  Finish the setup and run the crawler. It will create a new table in your `orders_db` database.

<img width="1440" height="900" alt="Screenshot 2025-11-12 at 4 10 20‚ÄØPM" src="https://github.com/user-attachments/assets/ddd00562-9844-4324-a684-f12d0a4b5f92" />
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 6 26 07‚ÄØPM" src="https://github.com/user-attachments/assets/aa4c4159-16a0-48f3-9e69-bd31b73b5161" />

---

## 6. Query Data with Amazon Athena üîç

Navigate to the **Athena** service. Ensure your data source is set to `AwsDataCatalog` and the database is `orders_db`. You can now run SQL queries on your processed data.

**Queries to be executed:**
* **Total Sales by Customer**: Calculate the total amount spent by each customer.
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 11 40 43‚ÄØPM" src="https://github.com/user-attachments/assets/a69c404e-c76a-4a00-88c3-6fbf8f78666f" />

* **Monthly Order Volume and Revenue**: Aggregate the number of orders and total revenue per month.
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 11 41 56‚ÄØPM" src="https://github.com/user-attachments/assets/b9e9cba6-5dc1-48d4-ace1-69056ce5d7bb" />

* **Order Status Dashboard**: Summarize orders based on their status (`shipped` vs. `confirmed`).
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 11 42 51‚ÄØPM" src="https://github.com/user-attachments/assets/763b7790-282b-4af8-b436-dffe4524ebc6" />

* **Average Order Value (AOV) per Customer**: Find the average amount spent per order for each customer.
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 11 43 24‚ÄØPM" src="https://github.com/user-attachments/assets/f8de0546-7d8e-4236-8946-84e35a402aae" />

* **Top 10 Largest Orders in February 2025**: Retrieve the highest-value orders from a specific month.
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 11 45 09‚ÄØPM" src="https://github.com/user-attachments/assets/cbd825b2-6629-4c43-ab86-ec5d336f47c5" />

* **Athena query CSV files in the S3 enriched folder**:
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 6 44 15‚ÄØPM" src="https://github.com/user-attachments/assets/feec3a8d-f4b1-495d-a484-75946ac571ae" />

---

## 7. Launch the EC2 Web Server üñ•Ô∏è

This instance will host a simple web page to display the Athena query results.

1.  Navigate to the **EC2** service and click **Launch instance**.
2.  **Name**: `Athena-Dashboard-Server`.
3.  **Application and OS Images**: Select **Amazon Linux 2023 AMI**.
4.  **Instance type**: Choose **t2.micro** (Free tier eligible).
5.  **Key pair (login)**: Create and download a new key pair. **Save the `.pem` file!**
6.  **Network settings**: Click **Edit** and configure the security group:
    * **Rule 1 (SSH)**: Type: `SSH`, Port: `22`, Source: `My IP`.
    * **Rule 2 (Web App)**: Click **Add security group rule**.
        * Type: `Custom TCP`
        * Port Range: `5000`
        * Source: `Anywhere` (`0.0.0.0/0`)
7.  **Advanced details**: Scroll down and for **IAM instance profile**, select the **EC2 Instance Profile** you created.
8.  Click **Launch instance**.

---

## 8. Connect to Your EC2 Instance

1.  From the EC2 dashboard, select your instance and copy its **Public IPv4 address**.
2.  Open a terminal or SSH client and connect using your key pair:

    ```bash
    ssh -i /path/to/your-key-file.pem ec2-user@YOUR_PUBLIC_IP_ADDRESS
    ```

---

## 9. Set Up the Web Environment

Once connected via SSH, run the following commands to install the necessary software.

1.  **Update system packages**:
    ```bash
    sudo yum update -y
    ```
2.  **Install Python and Pip**:
    ```bash
    sudo yum install python3-pip -y
    ```
3.  **Install Python libraries (Flask & Boto3)**:
    ```bash
    pip3 install Flask boto3
    ```

---

## 10. Create and Configure the Web Application

1.  Create the application file using the `nano` text editor:
    ```bash
    nano app.py
    ```
2.  Copy and paste your Python web application code (`EC2InstanceNANOapp.py`) into the editor.

3.  ‚ÄºÔ∏è **Important**: Update the placeholder variables at the top of the script:
    * `AWS_REGION`: Your AWS region (e.g., `us-east-1`).
    * `ATHENA_DATABASE`: The name of your Glue database (e.g., `orders_db`).
    * `S3_OUTPUT_LOCATION`: The S3 URI for your Athena query results (e.g., `s3://your-athena-results-bucket/`).

4.  Save the file and exit `nano` by pressing `Ctrl + X`, then `Y`, then `Enter`.

---

## 11. Run the App and View Your Dashboard! üöÄ

1.  Execute the Python script to start the web server:
    ```bash
    python3 app.py
    ```
    You should see a message like `* Running on http://0.0.0.0:5000/`.

2.  Open a web browser and navigate to your instance's public IP address on port 5000:
    ```
    http://YOUR_PUBLIC_IP_ADDRESS:5000
    ```
    You should now see your Athena Orders Dashboard!

<img width="1440" height="900" alt="Screenshot 2025-11-12 at 8 11 34‚ÄØPM" src="https://github.com/user-attachments/assets/3cb16eb5-2a41-4cd4-b1c7-2cde4979fa14" />
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 8 11 51‚ÄØPM" src="https://github.com/user-attachments/assets/277babac-c828-4cb4-ad10-9fb9ae3bd88d" />
<img width="1440" height="900" alt="Screenshot 2025-11-12 at 8 12 01‚ÄØPM" src="https://github.com/user-attachments/assets/032cb676-4db6-46d4-a189-f4864b676e65" />

---

## Important Final Notes

* **Stopping the Server**: To stop the Flask application, return to your SSH terminal and press `Ctrl + C`.
* **Cost Management**: This setup uses free-tier services. To prevent unexpected charges, **stop or terminate your EC2 instance** from the AWS console when you are finished.
